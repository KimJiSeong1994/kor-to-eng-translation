{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_gpu.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMJHEFF1VvVdgpWLUFSiY8a"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jUC_vYJGOA6c"},"source":["# Training on GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzhqKtLEOd4X","executionInfo":{"status":"ok","timestamp":1612185372526,"user_tz":-540,"elapsed":2031,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}},"outputId":"7d7b4ff3-1f51-484e-eef3-5b55792bbb0b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Programming/Projects/kor-to-eng-translation')\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","config\teval.py  README.md  train_gpu.ipynb  transformer\n","data\tlib\t results    train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUB_1ClxQfe4","executionInfo":{"status":"ok","timestamp":1612185375105,"user_tz":-540,"elapsed":4604,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}},"outputId":"e20d9be9-9dbd-496a-bc1f-0593e9f716ba"},"source":["!pip install konlpy "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OufwGR0lQUfN","executionInfo":{"status":"ok","timestamp":1612185375105,"user_tz":-540,"elapsed":4598,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}},"outputId":"ddb16b9c-8d75-4bbd-d297-5cd79610bf66"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["config\teval.py  README.md  train_gpu.ipynb  transformer\n","data\tlib\t results    train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3jxikdoQUL27","executionInfo":{"status":"ok","timestamp":1612185375869,"user_tz":-540,"elapsed":5358,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}}},"source":["#@title Transformer layers { display-mode: \"form\" }\n","\"\"\"\n","Word Embedding & Positional Embedding\n","\"\"\"\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","#####################################\n","\n","class Affine(nn.Module):\n","    \"\"\" Fully Connected Layer \"\"\"\n","\n","    def __init__(self, i_dim, o_dim):\n","        super(Affine, self).__init__()\n","        self.W = nn.Parameter(nn.init.xavier_normal_(torch.empty(i_dim, o_dim)))\n","        self.b = nn.Parameter(nn.init.uniform_(torch.empty(o_dim)))\n","\n","    def forward(self, inp, linear=False):\n","        \"\"\"\n","        Args:\n","            inp ([Tensor]): [bsize, maxlen, emb_size]\n","            linear (bool): bool\n","        Returns: [bsize, maxlen, hid_size]\n","        \"\"\"\n","        # [bsize, maxlen, emb_size] * [emb_size, hid_size]\n","        if linear:\n","            return torch.mm(inp, self.W) + self.b\n","        return F.relu((torch.matmul(inp, self.W)) + self.b)\n","\n","\n","class NormLayer(nn.Module):\n","    def __init__(self, d_inp, eps=1e-05):\n","        super(NormLayer, self).__init__()\n","        self.eps = eps\n","        self.gamma = Affine(d_inp, d_inp)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (Tensor): [bsize, maxlen, dim]\n","        Returns: [bsize, maxlen, dim]\n","        \"\"\"\n","        return self.gamma((x - torch.mean(x)) / torch.sqrt(torch.var(x) + self.eps))\n","\n","\n","class Attention(nn.Module):\n","    # 기존에 만들었던거 참조해서 만들기\n","    \"\"\" Scaled Dot-product Attention \"\"\"\n","\n","    def __init__(self, d_inp, d_q, d_k, d_v):\n","        super(Attention, self).__init__()\n","        self.Wq = Affine(d_inp, d_q)\n","        self.Wk = Affine(d_inp, d_k)\n","        self.Wv = Affine(d_inp, d_v)\n","\n","    def forward(self, query, key, value, mask=None):\n","        \"\"\"\n","        Args:\n","            query (): [bsize, maxlen, d_m]\n","            key (): [bsize, maxlen, d_m]\n","            value (): [bsize, maxlen, d_m]\n","            mask (): [bsize, ?, maxlen]\n","        Returns:  [bsize, maxlen, d_k]\n","        \"\"\"\n","        # [bsize, maxlen, d_k]\n","        wq = self.Wq(query)\n","        wk = self.Wk(key)\n","        wv = self.Wv(value)\n","        # attention distribution\n","        # Energy [bsize, maxlen, d_q] @ [bsize, d_k, maxlen] = [bsize, maxlen, maxlen]\n","        attn_dstr = torch.bmm(wq, torch.transpose(wk, 1, 2)) / torch.sqrt(torch.FloatTensor([key.size(-1)])).to(device)\n","        if mask is not None:\n","            attn_dstr = attn_dstr.masked_fill(mask == 0, -1e10)\n","        attn_dstr = F.softmax(attn_dstr, dim=2)\n","        # [bsize, maxlen, maxlen] @ [bsize, maxlen, d_v] = [bsize, maxlen, d_v]\n","        attn = torch.bmm(attn_dstr, wv)\n","        return attn\n","\n","\n","##############################\n","\n","class PositionalEmbedding(nn.Module):\n","    \"\"\"\n","    Basic Word Embedding\n","    Let the model learn sequence information with positional-encoding\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, emb_dim):\n","        super(PositionalEmbedding, self).__init__()\n","        # self.affine = Affine(vocab_size, emb_dim)\n","        self.embedding = WordEmbedding(vocab_size, emb_dim)\n","        self.dropout = nn.Dropout(p=0.1)\n","\n","    def forward(self, inp):\n","        \"\"\"\n","        Args:\n","            x (Tensor): [bsize, maxlen, emb_dim]\n","        Returns: [bsize, maxlen, emb_dim]\n","        \"\"\"\n","        \"\"\"\n","        임베딩값 dim 값으로 나눠주는거 놓침 \n","        \"\"\"\n","        # [bsize, maxlen, emb_dim]\n","        out = self.embedding(inp)\n","        # [bsize, maxlen, emb_dim]\n","        pe_rst = positional_embedding(out.size(0), out.size(1), out.size(2))\n","        # [bsize, maxlen, emb_dim]\n","        return self.dropout(out + pe_rst)\n","\n","\n","class WordEmbedding(nn.Module):\n","    def __init__(self, vocab_size, emb_dim):\n","        super(WordEmbedding, self).__init__()\n","        # self.affine = Affine(vocab_size, emb_dim)\n","        self.embedding = nn.Embedding(vocab_size, emb_dim)\n","\n","    def forward(self, inp):\n","        scale = torch.sqrt(torch.FloatTensor([inp.size(0)])).to(device)\n","        out = self.embedding(inp) / scale\n","        return out\n","\n","\n","def positional_embedding(bsize, maxlen, d_m):\n","    out = torch.stack(\n","        [positional_encoding(maxlen, d_m)] * bsize\n","    )\n","    return out.to(device)\n","\n","\n","def positional_encoding(maxlen, dim):\n","    \"\"\" Give unique value by position and dimension \"\"\"\n","\n","    def term(i):\n","        return 1 / (10000 ** (2 * (i // 2) / dim))\n","\n","    pos = torch.as_tensor(np.arange(maxlen))\n","    dims = np.arange(dim)\n","    dims = torch.tensor(list(map(lambda x: term(x), dims)))\n","    # [maxlen, dim]\n","    pe_val = pos.unsqueeze(1) * dims\n","    # [maxlen, dim]\n","    pe = torch.zeros(maxlen, dim)\n","    pe[:, 0::2] = torch.sin(pe_val[:, 0::2])\n","    pe[:, 1::2] = torch.cos(pe_val[:, 0::2])\n","    return pe.to(device)\n","\n","\n","########################\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" Multi-head Attention + Add&Norm \"\"\"\n","\n","    def __init__(self, d_m, n_head=4):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_m % n_head == 0\n","        d_k = int(d_m / n_head)\n","        self.head = n_head\n","        self.Wo = Affine(d_m, d_m)\n","        self.attn_layers = nn.ModuleList(\n","            [Attention(d_m, d_k, d_k, d_k) for _ in range(n_head)])\n","        self.dropout = nn.Dropout(p=0.01)\n","        self.addnorm = NormLayer(d_m)\n","\n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        Args:\n","            query (Tensor): [batch size, maxlen, d_m]\n","            key (Tensor): [batch size, maxlen, d_m]\n","            value (Tensor): [batch size, maxlen, d_m]\n","            mask (Tensor): [batch size, ?, maxlen]\n","        Returns: [batch size, maxlen, d_m]\n","        \"\"\"\n","        heads = []\n","        for layer in self.attn_layers:  # TODO 이게 맞는지 확인\n","            # head : [batch size, maxlen, d_k]\n","            head = layer(query, key, value, mask)\n","            heads.append(head)\n","        # [batch size, maxlen, d_k*head]\n","        multi_attn = self.Wo(torch.cat(heads, dim=2))\n","        multi_attn = self.dropout(multi_attn)\n","\n","        # [batch size, maxlen, d_k*head]\n","        resdl = query + multi_attn\n","        # [batch size, maxlen, d_k*head]\n","        out = self.addnorm(resdl)\n","        return out\n","\n","\n","class PositionWiseFFLayer(nn.Module):\n","    \"\"\" Position-wise FeedForward + Add&Norm \"\"\"\n","\n","    def __init__(self, d_m, d_ff):\n","        super(PositionWiseFFLayer, self).__init__()\n","        self.W1 = Affine(d_m, d_ff)\n","        self.W2 = Affine(d_ff, d_m)\n","        self.dropout = nn.Dropout(p=0.01)\n","        self.addnorm = NormLayer(d_m)\n","\n","    def forward(self, inp):\n","        \"\"\"\n","        Args:\n","            inp (Tensor): [batch size, maxlen, d_m]\n","        Returns: [batch size, maxlen, d_m]\n","        \"\"\"\n","        # [batch size, maxlen, d_ff]\n","        out = torch.relu(self.W1(inp))\n","        # [batch size, maxlen, d_m]\n","        out = self.W2(out)\n","        resdl = inp + self.dropout(out)\n","        # [batch size, maxlen, d_m]\n","        out = self.addnorm(resdl)\n","        return out\n","\n","#######################\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, d_m, d_ff):\n","        super(Encoder, self).__init__()\n","        self.multi_attn = MultiHeadAttention(d_m)\n","        self.pw_ff = PositionWiseFFLayer(d_m, d_ff)\n","\n","    def forward(self, inp, mask):\n","        \"\"\"\n","        Args:\n","            inp (Tensor): [batch size, maxlen, d_m]\n","            mask (Tensor): [batch size, 1, maxlen]\n","        Returns: [batch size, maxlen, d_m]\n","        \"\"\"\n","        # Sub-layer 1\n","        out = self.multi_attn(inp, inp, inp, mask)\n","        # Sub-layer 2\n","        out = self.pw_ff(out)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, inp_dim, d_m, d_ff):\n","        super(Decoder, self).__init__()\n","        self.multi_attn = MultiHeadAttention(d_m, inp_dim)\n","        self.multi_attn = MultiHeadAttention(d_m, inp_dim)\n","        self.pw_ff = PositionWiseFFLayer(d_m, d_ff)\n","\n","    def forward(self, inp, enc_out, src_mask, trg_mask):\n","        \"\"\"\n","        Args:\n","            inp (Tensor): [batch size, maxlen, d_m]\n","            enc_out (Tensor): [batch size, maxlen, d_m]\n","            src_mask (Tensor): [batch size, 1, maxlen]\n","            trg_mask (Tensor): [batch size, maxlen, maxlen]\n","        Returns:\n","        \"\"\"\n","        # Sub-layer 1\n","        # [batch size, maxlen, d_m]\n","        out = self.multi_attn(inp, inp, inp, trg_mask)  # masked self attention\n","        # Sub-layer 2\n","        # [batch size, maxlen, d_m]\n","        out = self.multi_attn(out, enc_out, enc_out, src_mask)  # encoder-decoder attention\n","        # Sub-layer 3\n","        # [batch size, maxlen, d_m]\n","        out = self.pw_ff(out)\n","        return out\n","\n","\n","\n","##############################\n","\n","\n","class BatchNorm(nn.Module):\n","    def __init__(self, num_feature, eps=0.01, momentum=0.9):  # maxlen\n","        super(BatchNorm, self).__init__()\n","        shape = 1, 1, num_feature  # (batch, maxlen, hidd), norm target is hidd\n","        self.eps = eps\n","        self.momentum = momentum\n","        self.gamma = nn.Parameter(nn.init.xavier_normal_(torch.empty(shape)))\n","        self.beta = nn.Parameter(nn.init.xavier_normal_(torch.empty(shape)))\n","\n","        # The variables that are not model parameters are initialized to 0\n","        self.moving_mean = torch.zeros(shape)\n","        self.moving_var = torch.zeros(shape)\n","\n","    def update_movings(self, mean, var):\n","        self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * mean\n","        self.moving_var = self.momentum * self.moving_var + (1 - self.momentum) * var\n","\n","    def forward(self, batch):\n","        # If `X` is not on the main memory, copy `moving_mean` and\n","        # `moving_var` to the device where `X` is located\n","        if not torch.is_grad_enabled():\n","            self.moving_mean = self.moving_mean.to(batch.device)\n","            self.moving_var = self.moving_var.to(batch.device)\n","            normed = (batch - self.moving_mean) / torch.sqrt(self.moving_var + self.eps)\n","        else:\n","            mean = torch.mean(batch, dim=(0, 1), keepdim=True)\n","            var = torch.var(batch, dim=(0, 1), keepdim=True)\n","            normed = (batch - mean) / torch.sqrt(var + self.eps)\n","            self.update_movings(mean, var)\n","        new_batch = self.gamma * normed + self.beta\n","        return new_batch\n","\n","\n","class LabelSmoothingLoss(nn.NLLLoss):\n","    def __init__(self, a: float = 0.01, reduction='mean', ignore_index=-100):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.a = a\n","        self.reduction = reduction\n","        self.ignore_index = ignore_index\n","\n","    @torch.no_grad()\n","    def forward(self, pred, trg):\n","        K = pred.size(-1)  # class number\n","        trg_idx = trg != self.ignore_index  # identify not PAD\n","        trg = trg[trg_idx]\n","\n","        log_pred = F.log_softmax(pred[trg_idx], dim=-1)\n","        loss = -torch.sum(log_pred, dim=-1)\n","        if self.reduction == 'mean':\n","            loss = torch.mean(loss)\n","        elif self.reduction == 'sum':\n","            loss = torch.sum(loss)\n","        nll_loss = F.nll_loss(log_pred, trg, reduction=self.reduction)\n","        loss = nll_loss * (1 - self.a) + self.a * (loss / K)\n","        return loss.mean()\n","\n","\n","class CrossEntropyLoss(nn.Module):\n","    def __init__(self, a: float = 0.01, reduction='mean', ignore_index=-100):\n","        super(CrossEntropyLoss, self).__init__()\n","        self.a = a\n","        self.reduction = reduction\n","        self.ignore_index = ignore_index\n","\n","    @torch.no_grad()\n","    def forward(self, pred, trg):\n","        pass\n","\n","\n","\n","###################\n","\n","\n","class Transformer(nn.Module):\n","    \"\"\" Assemble layers to build Transformer \"\"\"\n","\n","    def __init__(self, d_m, inp_vocab_size, out_vocab_size, d_ff, n=3):\n","        super(Transformer, self).__init__()\n","        self.inp_emb = PositionalEmbedding(inp_vocab_size, d_m)\n","        self.out_emb = PositionalEmbedding(out_vocab_size, d_m)\n","        self.enc_layers = nn.ModuleList(\n","            [Encoder(d_m, d_ff) for _ in range(n)])\n","        self.dec_layers = nn.ModuleList(\n","            [Decoder(d_m, d_m, d_ff) for _ in range(n)])\n","        self.affine = Affine(d_m, out_vocab_size)\n","        self.n = n\n","\n","    def encoder(self, inp_batch, src_mask):\n","        \"\"\"\n","        Args:\n","            inp_batch (Tensor): [batch size, maxlen]\n","            src_mask (Tensor): [bsize, 1, maxlen]\n","        Returns: [batch size, maxlen, d_m]\n","        \"\"\"\n","        # [batch size, maxlen, d_m]\n","        i_emb = self.inp_emb(inp_batch)\n","        # Encoder\n","        enc = i_emb\n","        for layer in self.enc_layers:\n","            # [batch size, maxlen, d_m]\n","            enc = layer(enc, src_mask)\n","        return enc\n","\n","    def forward(self, inp_batch, out_batch):\n","        \"\"\"\n","        Args:\n","            inp_batch (Tensor): [batch size, maxlen]\n","            out_batch (Tensor): [batch size, maxlen]\n","        Returns: [batch size, maxlen, vocab_size]\n","        \"\"\"\n","        # Encoder\n","        src_mask = mask_not_pad(inp_batch)\n","        # [batch size, maxlen, d_m]\n","        enc = self.encoder(inp_batch, src_mask)\n","\n","        # Decoder\n","        trg_mask = mask_get_dec(out_batch)\n","        # [batch size, maxlen, d_m]\n","        o_emb = self.out_emb(out_batch)\n","        dec = o_emb\n","        for layer in self.dec_layers:\n","            # [batch size, maxlen, d_m]\n","            dec = layer(dec, enc, src_mask, trg_mask)\n","        # [batch size, maxlen, vocab_size]\n","        rst = F.log_softmax(self.affine(dec), dim=2)\n","        return rst\n","\n","    @torch.no_grad()\n","    def predict(self, inp_batch):\n","        \"\"\"\n","        Args:\n","            inp_batch (Tensor): [batch size, maxlen]\n","        Returns: [batch size, maxlen, vocab_size]\n","        \"\"\"\n","        src_mask = mask_not_pad(inp_batch)\n","        # [batch size, maxlen, d_m]\n","        enc = self.encoder(inp_batch, src_mask)\n","        # [batch size, maxlen, d_m] @ [d_m, vocab_size]\n","        # => [batch size, maxlen, vocab_size]\n","        rst = F.log_softmax(self.affine(enc), dim=2)\n","        rst = torch.argmax(rst, dim=-1).tolist()\n","        return rst\n","\n","\n","def mask_not_pad(x):\n","    \"\"\"\n","    Mark True at PAD\n","    Args:\n","        x (Tensor): [bsize, maxlen] with word idx\n","    Returns: [bsize, 1, maxlen] with bool if idx <=0, True\n","    \"\"\"\n","    return (x > 0).unsqueeze(1).to(device)\n","\n","\n","def mask_get_dec(x):\n","    \"\"\"\n","    Mark dec right sequence\n","    Args:\n","        x (Tensor): [bsize, maxlen] with bool\n","    Returns: [bsize, maxlen, maxlen] with bool\n","    \"\"\"\n","    # [bsize, 1, maxlen]\n","    pad_masked = mask_not_pad(x)\n","    # [maxlen, maxlen]\n","    seq_masked = torch.tril(torch.ones(x.size(1), x.size(1))).to(device)\n","    # [bsize, maxlen, maxlen]\n","    seq_masked = seq_masked.unsqueeze(0).repeat(x.size(0), 1, 1)\n","    # [bsize, maxlen, maxlen]\n","    masked = seq_masked.masked_fill(pad_masked == 0, 0)\n","    return masked.to(device)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfoRDSpsaEYn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612185377159,"user_tz":-540,"elapsed":6644,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}},"outputId":"a18f77d8-e3cb-4506-e092-dfd453fa2568"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import math\n","\n","from lib.data_batchify import TrainCorpus, collate_fn\n","from lib.data_preprocess import preprocessor\n","\n","\n","@torch.no_grad()\n","def accuracy(pred, target):\n","    acc = sum(pred.argmax(1) == target).item() / len(target)\n","    return acc\n","\n","\n","class LangTranslator:\n","    def __init__(self, model, ko_vocab, en_vocab, dconf, mconf, device):\n","        self.dconf = dconf\n","        self.mconf = mconf\n","\n","        self.ko_vocab = ko_vocab\n","        self.en_vocab = en_vocab\n","        self.dataset = None\n","        self.dataload = None\n","\n","        self.device = device\n","        self.model = model\n","\n","        self.loss = nn.CrossEntropyLoss()\n","        self.optim = optim.Adam(params=self.model.parameters(), lr=self.mconf.lr)\n","        self.lrscheder = optim.lr_scheduler.ReduceLROnPlateau(self.optim, patience=5)\n","\n","    def train(self, ko_corpus, en_corpus):\n","        train_set = self.trainset_form(ko_corpus, en_corpus, self.ko_vocab, self.en_vocab)\n","        self.dataset = TrainCorpus(train_set)\n","        self.dataload = DataLoader(self.dataset,\n","                                   batch_size=self.mconf.batch_size,\n","                                   num_workers=0, collate_fn=collate_fn)\n","        self.mconf.ko_size, self.mconf.en_size = len(self.ko_vocab) + 1, len(self.en_vocab) + 1\n","\n","        total_loss = 0\n","        total_acc = 0\n","        self.model.train()\n","        # self.info()\n","        for epoch in tqdm(range(self.mconf.epoch), desc='epoch'):\n","            for i, batch in tqdm(enumerate(self.dataload), desc=\"step\", total=len(self.dataload)):\n","                ko, en = map(lambda ds: ds.to(self.device), batch)\n","                self.optim.zero_grad()\n","                en_xs = en[:, :-1]\n","                en_ts = en[:, 1:]\n","                pred = self.model(ko, en_xs)\n","                pred, en_ts = pred.view(-1, pred.shape[2]), en_ts.reshape(1, -1).squeeze(0)\n","                b_loss = self.loss(pred, en_ts)\n","                b_loss.backward()\n","                self.optim.step()\n","\n","                total_acc += accuracy(pred, en_ts)\n","                total_loss += b_loss.item()\n","                del ko, en, en_xs, en_ts, pred\n","                torch.cuda.empty_cache()\n","\n","\n","            itersize = math.ceil(len(self.dataset) / self.mconf.batch_size)\n","            ppl = math.exp(total_loss / itersize)\n","            print(epoch, total_loss, total_acc / itersize, ppl)\n","            self.lrscheder.step(total_loss)\n","            total_loss = 0\n","        self.en_vocab.to_idx2word()\n","\n","    def trainset_form(self, ko_corpus, en_corpus, ko_vocab, en_vocab):\n","        \"\"\" form train data - word to idx \"\"\"\n","        rst = []\n","        for ko, en in zip(ko_corpus, en_corpus):\n","            ko = [ko_vocab[x] for x in ko]\n","            en = [en_vocab[x] for x in en]\n","            rst.append([ko, en])\n","        return rst\n","\n","    def predset_form(self, corpus, vocab):\n","        \"\"\" form evaluate data - word to idx \"\"\"\n","        rst = []\n","        for ko in corpus:\n","            ko = [vocab[x] for x in ko]\n","            rst.append(ko)\n","        return rst\n","\n","    def predict(self, corpus):\n","        \"\"\" predict trained model \"\"\"\n","        ko_corpus = preprocessor(corpus, lang='ko')\n","        pred_set = self.predset_form(ko_corpus, self.ko_vocab)\n","        pred_set = [torch.tensor(data) for data in pred_set]\n","        dataset = torch.nn.utils.rnn.pad_sequence(pred_set, batch_first=True)\n","        pred = self.model.predict(dataset, maxlen=dataset.size(1))\n","        return pred\n","\n","    def translate(self, kor: list):\n","        \"\"\" Translate Korean to English \"\"\"\n","        pred = self.predict(kor)\n","        rst = []\n","        for sent_idx in pred:\n","            sent = [self.en_vocab.get_word(idx) for idx in sent_idx if not 0]\n","            rst.append(sent)\n","        return rst\n","\n","    def save(self, fname: str):\n","        \"\"\" save model \"\"\"\n","\n","        torch.save({\n","            'model': self.model.state_dict(),\n","            'optim': self.optim.state_dict(),\n","            'ko_vocab': self.ko_vocab,\n","            'en_vocab': self.en_vocab\n","        }, 'results/model/' + fname)\n","\n","    def load(self, fname: str, retrain=False):\n","        \"\"\" load model \"\"\"\n","        if not self.model:\n","            raise\n","        checkpoint = torch.load('results/model/' + fname)\n","        self.model.load_state_dict(checkpoint['model'])\n","        if self.optim and retrain:\n","            self.optim.load_state_dict(checkpoint['optim'])\n","        self.ko_vocab = checkpoint['ko_vocab']\n","        self.en_vocab = checkpoint['en_vocab']\n","        self.en_vocab.to_idx2word()\n","        self.model.eval()\n","        print(len(self.ko_vocab), len(self.en_vocab))\n","\n","    def __repr__(self):\n","        print(\"Model's state_dict:\")\n","        for param_tensor in self.model.state_dict():\n","            print(param_tensor, \"\\t\", self.model.state_dict()[param_tensor].size())\n","\n","        print(\"Optimizer's state_dict:\")\n","        for var_name in self.optim.state_dict():\n","            print(var_name, \"\\t\", self.optim.state_dict()[var_name])\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":494},"id":"idv2mbG7OZrV","executionInfo":{"status":"error","timestamp":1612185494788,"user_tz":-540,"elapsed":124260,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}},"outputId":"26dd4365-bcdd-4b60-dceb-f7d5d6b2fc33"},"source":["import torch\n","# from transformer.lib.model.transformer import Transformer\n","\n","from lib.util import Config\n","# from lib.kor2eng import LangTranslator\n","from lib.util import load_data\n","from lib.data_preprocess import Vocab, preprocessor\n","from lib.model.seq2seq import BiLSTMSeq2Seq\n","\n","# import os\n","# cwd = os.getcwd()\n","# print(cwd)\n","\n","# import os\n","# arr = os.listdir()\n","# print(arr)\n","\n","# load configs\n","dconf_path = 'config/data.json'\n","mconf_path = 'config/lm.json'\n","dconf = Config(dconf_path)\n","mconf = Config(mconf_path)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","print('Using device:', device)\n","\n","ko_data = load_data(dconf.train_ko_path)\n","en_data = load_data(dconf.train_en_path)\n","print(len(ko_data), len(en_data))\n","# load & preprocess corpus\n","ko_corpus = preprocessor(ko_data[:int(len(ko_data)/2)], lang='ko')\n","en_corpus = preprocessor(en_data[:int(len(en_data)/2)], lang='en')\n","\n","# load vocab\n","ko_vocab = Vocab(dconf.min_cnt)\n","en_vocab = Vocab(dconf.min_cnt)\n","ko_vocab.load(ko_corpus)\n","en_vocab.load(en_corpus)\n","\n","# # define lm model\n","# if mconf.model == 'transformer':\n","#     model = Transformer(mconf.d_m, len(ko_vocab) + 1, len(en_vocab) + 1,\n","#                         mconf.d_m * 4, n_layer=3)\n","# else:\n","#     model = BiLSTMSeq2Seq(len(ko_vocab) + 1, len(en_vocab) + 1,\n","#                           mconf.emb_dim, mconf.d_m)\n","model = Transformer(mconf.d_m, len(ko_vocab) + 1, len(en_vocab) + 1, mconf.d_m * 4, n=3)\n","model.to(device)\n","\n","# load translator and train\n","lm = LangTranslator(model, ko_vocab, en_vocab, dconf, mconf, device)\n","lm.train(ko_corpus, ko_corpus)\n","\n","# save model\n","lm.save('trained.pth')\n","mconf.save(mconf_path)\n","\n","test = ['또 하나 필요한 것은 훌륭한 영어 실력이다.',\n","        '경찰은 월요일 밤 집무실을 찾아 증거를 압수했다.']\n","print(lm.translate(test))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using device: cuda\n","5000 5000\n"],"name":"stdout"},{"output_type":"stream","text":["epoch:   0%|          | 0/100 [00:00<?, ?it/s]\n","step:   0%|          | 0/6 [00:00<?, ?it/s]\n","epoch:   0%|          | 0/100 [00:00<?, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-48c26978b37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# load translator and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLangTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mko_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mko_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-dd2c04425617>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ko_corpus, en_corpus)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0men_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0men_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-95c8ca52c045>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp_batch, out_batch)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# [batch size, maxlen, d_m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;31m# [batch size, maxlen, vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-95c8ca52c045>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, enc_out, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Sub-layer 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# [batch size, maxlen, d_m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# encoder-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Sub-layer 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# [batch size, maxlen, d_m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-95c8ca52c045>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_layers\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO 이게 맞는지 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# head : [batch size, maxlen, d_k]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mheads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# [batch size, maxlen, d_k*head]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-95c8ca52c045>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# attention distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Energy [bsize, maxlen, d_q] @ [bsize, d_k, maxlen] = [bsize, maxlen, maxlen]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mattn_dstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mattn_dstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_dstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.75 GiB total capacity; 13.24 GiB already allocated; 10.88 MiB free; 14.53 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"VUrqZUBfqNev","executionInfo":{"status":"aborted","timestamp":1612185494789,"user_tz":-540,"elapsed":124256,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZiibOrIriL5","executionInfo":{"status":"aborted","timestamp":1612185494790,"user_tz":-540,"elapsed":124254,"user":{"displayName":"hansol park","photoUrl":"","userId":"13314081940074376337"}}},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]}]}